{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m       \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m f\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAnnotation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Annotation'"
     ]
    }
   ],
   "source": [
    "import pandas    as pd\n",
    "import geopandas as geopd\n",
    "import seaborn   as sea \n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql       import SparkSession, dataframe\n",
    "from pyspark.sql       import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from Annotation import annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & AI - PB Fast Learning - Ciência de Dados - 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Neste projeto ficarei usando pandas, pyspark([sql]()),  pyspark([dataframe]()) mais vou utilizar mais as duas ultimas pois já tenho um certo conhecimento em Pandas e quero aprender|fixar o conhecimento sobre o Pyspark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Seção no spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local')\\\n",
    "    .appName('ProjetoSprint01')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados usando pyspark\n",
    "> Defini o tipo do Schema(com o `StructType`) para alguns Conj. dados pois estavam tipando errado o schema, por exemplo, **`geolocation_zip_code_prefix`** que seus atributos são do tipo string contedo só numero de caracteres, mas estavam sendo colocados como do tipo `integer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# olist_customers_dataset\n",
    "customers     = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_customers_dataset.csv')\n",
    "\n",
    "# olist_geolocation_dataset\n",
    "schemaGeo = StructType([ \n",
    "    StructField(\"geolocation_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"geolocation_lat\", DoubleType(), True),\n",
    "    StructField(\"geolocation_lng\", DoubleType(), True),\n",
    "    StructField(\"geolocation_city\", StringType(), True),\n",
    "    StructField(\"geolocation_state\", StringType(), True)\n",
    "])\n",
    "geolocation   = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').schema(schemaGeo)\\\n",
    "        .load('dataset/olist_geolocation_dataset.csv')\n",
    "\n",
    "# olist_order_payments_dataset\n",
    "payments      = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_order_payments_dataset.csv')\n",
    "\n",
    "# olist_orders_dataset\n",
    "orders        = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_orders_dataset.csv')\n",
    "\n",
    "# olist_order_items_dataset\n",
    "order_items   = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_order_items_dataset.csv')\n",
    "\n",
    "# olist_order_reviews_dataset\n",
    "order_reviews = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_order_reviews_dataset.csv')\n",
    "\n",
    "# olist_products_dataset\n",
    "order_product = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/olist_products_dataset.csv')\n",
    "\n",
    "# olist_sellers_dataset\n",
    "schemaSeller = StructType([\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"seller_zip_code_prefix\", StringType(), True),\n",
    "    StructField(\"seller_city\", StringType(), True),\n",
    "    StructField(\"seller_state\", StringType(), True)\n",
    "])\n",
    "sellers      = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').schema(schemaSeller)\\\n",
    "        .load('dataset/olist_sellers_dataset.csv')\n",
    "\n",
    "# product_category_dataset\n",
    "product_cat  = spark.read.format('csv')\\\n",
    "    .option('header','True').option('delimiter',',').option('inferschema','True')\\\n",
    "        .load('dataset/product_category_name_translation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma visão temporaria , pois usarei o sql do spark para fazer as questões e melhor de entender oq estou fazendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.createOrReplaceTempView('customers')\n",
    "geolocation.createOrReplaceTempView('geolocation')\n",
    "orders.createOrReplaceTempView('orders')\n",
    "order_items.createOrReplaceTempView('order_items')\n",
    "order_product.createOrReplaceTempView('order_product')\n",
    "order_reviews.createOrReplaceTempView('order_reviews')\n",
    "payments.createOrReplaceTempView('payments')\n",
    "sellers.createOrReplaceTempView('sellers')\n",
    "product_cat.createOrReplaceTempView('product_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quais são categorias com maior e menor receita dos ultimos 12 meses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select min(order_purchase_timestamp), max(order_purchase_timestamp) from orders;'\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> O data vão de 2016 a 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando **Sql Spark** para fazer as questões \\\n",
    "Fazendo o `INNER JOIN` nas repestivas tabelas abaixo e com o `order_status!=cancelado` ,pois so será computado as vendas feitas(ou seja diferente de cancelado) assim consigo obter o valor real das Receitas de um produto, vender, estado.....\n",
    "> Essa logica será feitas sobre as questões adiante que pedem `receitas` de algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT product_category_name, SUM(payment_value) as Receita FROM order_items as o_i\n",
    "    INNER JOIN order_product as o_p ON\n",
    "        o_p.product_id = o_i.product_id\n",
    "    INNER JOIN orders as or ON\n",
    "        or.order_id = o_i.order_id\n",
    "    INNER JOIN payments as pay ON\n",
    "        pay.order_id = or.order_id\n",
    "\n",
    "    WHERE order_status != 'canceled' and\n",
    "        order_purchase_timestamp >= '2017-10-15 01:00:00' and\n",
    "        order_purchase_timestamp <= '2018-10-17 17:30:18'\n",
    "    GROUP BY product_category_name\n",
    "    ORDER BY receita desc;\n",
    "'''\n",
    "data = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando a coluna `product_category_name` para torna-la mais legivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirar o `_` e colacando o ' ' (espaco vazio) usando o replace\n",
    "data['product_category_name'] = data['product_category_name'].str.replace('_',' ')\n",
    "# Deixando as primeiras letras de cada palavra maiuscula\n",
    "data['product_category_name'] = data['product_category_name'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usarei seaborn e matplotlib para visualização pois ele renderiza no github, diferente do plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.set_theme(context='notebook',style='ticks')  # Definindo estilo\n",
    "fig, ax = plt.subplots(ncols=1,nrows=2,figsize=(18,20))\n",
    "fig.subplots_adjust(hspace=0.25,right=1)        # Aumenta o espaço entre os graficos\n",
    "\n",
    "# Grafico 1 = Melhores receitas\n",
    "barMelhores = sea.barplot( data = data.iloc[0:10,:], \n",
    "    y='product_category_name', x='Receita', hue='product_category_name',width=0.85,\n",
    "    palette=sea.color_palette('pastel6',n_colors=10), ax = ax[0], legend=False\n",
    ")\n",
    "\n",
    "# Grafico 2 = Piores receitas\n",
    "barPiores = sea.barplot(data=data.iloc[63:76,:], \n",
    "    y='product_category_name', x='Receita', hue='product_category_name',width=0.85,\n",
    "    palette =  sea.color_palette('flare',n_colors=10), ax = ax[1], legend=False\n",
    ")\n",
    "\n",
    "#Colocar text nas barras do graphico fiz dessa forma para\n",
    "#economizar o numero de linhas\n",
    "for graph,ha in zip([barMelhores,barPiores],['right','left']):\n",
    "    for p in graph.patches:\n",
    "        if p.get_width() != 0:\n",
    "            xy = (p.get_width(), p.get_y()+p.get_height()/2)\n",
    "            graph.annotate(\"%.2f\" % p.get_width(),xy = xy, \n",
    "                xytext = (0, 0.1), textcoords= 'offset points', ha=ha, va = \"center\" ,size=15)\n",
    "\n",
    "# Modificando Layout do grafico para deixar mais bonito\n",
    "for axis in ax.flatten():\n",
    "    axis.set_xlabel('')\n",
    "    axis.set_ylabel('Categoria de Produtos',x=10,fontsize=17)\n",
    "    axis.spines[['right','top','bottom']].set_visible(False)\n",
    "    axis.tick_params('y', labelsize=16)\n",
    "    axis.tick_params('x', labelsize=16)         \n",
    "    axis.set_axisbelow(True)\n",
    "    axis.grid(True)\n",
    "\n",
    "#Modificando o Grafico 1\n",
    "ax[0].set_title('Melhores Receitas',y=1.05,fontsize=22)\n",
    "\n",
    "#Modificando o Grafico 2\n",
    "ax[1].set_title('Piores Receitas',y=1.06,fontsize=22)       # Titulo \n",
    "ax[1].set_xlabel('Receita',fontsize=17)                     # Modificando eixo x\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 maiores sellers (maior receita)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT (INITCAP(se.seller_city)|| '-' || se.seller_state) as seller ,\n",
    "        ROUND(sum(payment_value),2) as receita FROM sellers AS se\n",
    "    INNER JOIN order_items AS o_i ON\n",
    "        o_i.seller_id = se.seller_id\n",
    "    INNER JOIN payments AS py ON\n",
    "        py.order_id  = o_i.order_id\n",
    "    INNER JOIN orders AS o_r ON\n",
    "        o_r.order_id = py.order_id\n",
    "    WHERE order_status != 'canceled'  \n",
    "    GROUP BY seller\n",
    "    ORDER BY receita desc limit 10; \n",
    "'''\n",
    "data = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.set_theme(style=\"ticks\")                # Definindo estilo\n",
    "fig, ax = plt.subplots(ncols=1,nrows=1,figsize=(18,10))\n",
    "fig.subplots_adjust(hspace=0.25,right=1)    # Aumenta o espaço entre os graficos\n",
    "\n",
    "# Grafico 1 = Melhores receitas\n",
    "barMelhores = sea.barplot( data = data.iloc[0:10,:], \n",
    "    y='seller', x='receita', hue='seller', width=0.9, ax = ax, legend=False )\n",
    "\n",
    "#Modificando o Grafico 1\n",
    "ax.set_title('Melhores receitas de municipios Total dos anos de 2016 a 2018',y=1.05,fontsize=20)\n",
    "ax.set_ylabel('Seller',x=10,fontsize=17)\n",
    "ax.set_xlabel('Receitas',fontsize=17)\n",
    "ax.spines[['right','top','bottom']].set_visible(False)\n",
    "ax.tick_params('y', labelsize=16)\n",
    "ax.tick_params('x', labelsize=16)         \n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(True)\n",
    "\n",
    "# Colocando o valor das receitas nas barras\n",
    "for p in barMelhores.patches:\n",
    "    if p.get_width() != 0:\n",
    "        barMelhores.annotate((\"%.2f\" % p.get_width()),  xy = (p.get_width(), p.get_y()+p.get_height()/2), \n",
    "        xytext = (0, 0.1), textcoords= 'offset points', ha = 'right', va = \"center\" ,size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT seller_state, order_approved_at, payment_value FROM sellers AS se \n",
    "    INNER JOIN order_items AS o_i ON\n",
    "        o_i.seller_id = se.seller_id\n",
    "    INNER JOIN payments AS py ON\n",
    "        py.order_id  = o_i.order_id\n",
    "    INNER JOIN orders AS o_r ON\n",
    "        o_r.order_id = py.order_id;\n",
    "'''\n",
    "dataSql = spark.sql(query).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vou usar o spark para fazer essa operação, depois passarei para pandas para visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSpark = dataSql.groupby(\n",
    "    [f.col('seller_state') ,f.year( f.col('order_approved_at') ).alias('ano')] )\\\n",
    "    .agg( \n",
    "        f.round(f.sum(f.col('payment_value'))).alias('receita') )\\\n",
    "    .orderBy( \n",
    "        f.col('receita').desc() )\n",
    "dataPd = dataSpark.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando usando um loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.set_theme('paper',style='ticks')\n",
    "fig, ax = plt.subplots(ncols=1,nrows=3, figsize=(11,14))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for ano,axis in zip(range(2016, 2019),ax.flatten()):\n",
    "    selecti = dataPd.loc[ dataPd['ano'] == ano ].iloc[0:10,:]\n",
    "\n",
    "    bar = sea.barplot(\n",
    "        data=selecti, y='receita', x='seller_state', hue='seller_state',ax=axis,\n",
    "        palette=sea.color_palette('muted6',n_colors=10) , width=1,gap=0.1\n",
    "    )\n",
    "    axis.set_title(f'Receitas dos 10 Melhores Seller no ano {ano}',fontsize=17)\n",
    "    axis.set_ylabel('Estados',fontsize=12)\n",
    "    axis.set_xlabel('')\n",
    "    axis.tick_params('y', labelsize=10)\n",
    "    axis.spines[['right','top','left']].set_visible(False)\n",
    "    axis.grid(axis='y')\n",
    "\n",
    "    for i in bar.containers: bar.bar_label(i,fontsize=10)\n",
    "axis.set_xlabel('Receitas',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 piores sellers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existem sellers que vendem o mesmo produto? se sim quais são? Qual a variação de preço praticada entre os sellers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usarei o pandas para fazer o group by pois fica melhor de visualizar a tabela ou ela organiza melhor quando usa duas colunas para essa operação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT seller_state, product_category_name,price FROM sellers AS se\n",
    "    INNER JOIN order_items AS o_i ON\n",
    "        se.seller_id = o_i.seller_id\n",
    "    INNER JOIN order_product AS op ON\n",
    "        op.product_id = o_i.product_id\n",
    "'''\n",
    "data = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = data.groupby(by=['seller_state', 'product_category_name'])['price'].mean().reset_index()\n",
    "resultado.dropna(inplace=True)\n",
    "resultado.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarianciaDeLojas(data:pd.DataFrame):\n",
    "    dicionario = {'States':[],'Produto':[],'Variação':[]}\n",
    "    for enum ,statei in data.iterrows():\n",
    "        for _ ,statej in data.iloc[enum:,:].iterrows():\n",
    "            if (statei['seller_state'] != statej['seller_state']):\n",
    "                if (statei['product_category_name'] == statej['product_category_name']):\n",
    "                    dicionario['Produto'].append(statei['product_category_name'])\n",
    "                    dicionario['States'].append(statei['seller_state'] +'-'+statej['seller_state'])\n",
    "                    variacao =  ((statei['price'] - statej['price']))\n",
    "                    dicionario['Variação'].append(variacao)\n",
    "    return pd.DataFrame(dicionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ouve inflação de preço dos produtos ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * FROM order_product AS op\n",
    "    INNER JOIN order_items AS o_i ON\n",
    "        op.product_id = o_i.product_id;\n",
    "'''\n",
    "data = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = pd.to_datetime(data['shipping_limit_date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvGlobal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
